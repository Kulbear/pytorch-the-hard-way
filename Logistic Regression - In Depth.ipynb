{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Regression is the hammer we reach for when we want to answer *how much?* or *how many?* questions.\n",
    "If you want to predict the number of dollars (the *price*) at which a house will be sold,\n",
    "or the number of wins a baseball team might have, \n",
    "or the number of days that a patient will remain hospitalized before being discharged,\n",
    "then you're probably looking for a regression model.\n",
    "\n",
    "Based on our experience, in industry, we're more often interested in making categorical assignments.\n",
    "*Does this email belong in the spam folder or the inbox*?\n",
    "*How likely is this custromer to sign up for subscription service?*\n",
    "When we're interested in either assigning datapoints to categories\n",
    "or assessing the *probability* that a category applies,\n",
    "we call this task *classification*. \n",
    "\n",
    "The simplest kind of classification problem is *binary classification*,\n",
    "when there are only two categories,\n",
    "so let's start there. \n",
    "Let's call our two categories the positive class $y_i=1$ and the negative class $y_i = 0$.\n",
    "Even with just two categories, and even confining ourselves to linear models, \n",
    "there are many ways we might approach the problem. \n",
    "For example, we might try to draw a line that best separates the points.\n",
    "\n",
    "A whole family of algorithms called support vector machines pursue this approach.\n",
    "The main idea here is choose a line that maximizes the margin to the closest data points on either side of the decision boundary. \n",
    "In these approaches, only the points closest to the decision boundary (the support vectors) \n",
    "actually influence the choice of the linear separator.\n",
    "\n",
    "With neural networks, we usually approach the problem differently. \n",
    "Instead of just trying to separate the points,\n",
    "we train a probabilistic classifier which estimates,\n",
    "for each data point, the conditional probability that it belongs to the positive class. \n",
    "\n",
    "Recall that in linear regression, we made predictions of the form\n",
    "\n",
    "$$ \\hat{y} = \\boldsymbol{w}^T \\boldsymbol{x} + b. $$\n",
    "\n",
    "We are interested in asking the question *\"what is the probability that example $x$ belongs to the positive class?\"*\n",
    "A regular linear model is a poor choice here because it can output values greater than $1$ or less than $0$.\n",
    "To coerce reasonable answers from our model, \n",
    "we're going to modify it slightly,\n",
    "by running the linear function through a sigmoid activation function $\\sigma$:\n",
    "\n",
    "$$ \\hat{y} =\\sigma(\\boldsymbol{w}^T \\boldsymbol{x} + b). $$\n",
    "\n",
    "The sigmoid function $\\sigma$, sometimes called a squashing function or a *logistic* function - thus the name logistic regression - maps a real-valued input to the range 0 to 1.\n",
    "Specifically, it has the functional form:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Let's get our imports out of the way and visualize the logistic function using `PyTorch` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl01fWd//HnO/seliRsSVgk7LgRWXSqtm6IVrs4KopLXbCntbVTq3Xpz+lop63aqXU6thZx6o5FrZZaKlVrRSsgIPsOYUkIkI3s600+80eiv4jBXMJNvnd5Pc7h5C5fbl73JHmdTz75fD9fc84hIiLhJcrrACIiEngqdxGRMKRyFxEJQyp3EZEwpHIXEQlDKncRkTCkchcRCUMqdxGRMKRyFxEJQzFefeKMjAw3YsQIrz69iEhIWr16dZlzLrO74zwr9xEjRrBq1SqvPr2ISEgys73+HKdpGRGRMKRyFxEJQyp3EZEw1G25m9n/mlmJmW08yvNmZv9tZjvNbL2ZnRr4mCIiciz8Gbk/Bcz8nOcvBPI6/s0Ffnv8sURE5Hh0W+7OuaVAxecccinwjGu3HOhnZkMCFVBERI5dIObchwGFne4XdTwmIiIeCcQ6d+visS6v3Wdmc2mfuiE3NzcAn1pEJHi0tTlqmnxUN7RQ0+ijtslHTWMLtU3tt+uafNQ2tXLOuCxOyunXq1kCUe5FQE6n+9lAcVcHOufmAfMA8vPzdfFWEQlazjmqG3yU1jZRVttEeW0z5XXtHw/XN3O4voXK+vbbVQ0tVNa3l7g/l6XOSo0PiXJfBNxqZi8C04Aq59yBALyuiEivcM5RVtvM/soG9h9u4EBVAweqGjlY1cjB6kYOVTdSUtNEs6+ty/+fnhhL/6RY+ifHkZkST15WKumJsaQlxpKWEPPJx9SEWFLiY0iOjyE1of1jUmw0UVFdTXgEVrflbmYLgLOBDDMrAv4diAVwzj0OLAZmATuBeuAbvRVWRMRfbW2O4qoGdpfVsaesjoKyOvaV17Ovop7Cw/U0tny6uBNjoxnSL4FBqQnkD+/PoLQEMlPjyUyNJyMlnoEpcQxMjqd/Uiwx0cF/ilC35e6cm93N8w74dsASiYgcA+ccJTVNbDlQzdaDNWw/WMOOklp2ltTS0NL6yXFJcdHkDkhiZEYyZ43JJLt/IsP6JzGsXyLD+iWSlhiDWe+PqPuKZxuHiYj0REl1I2sKK1lXWMnG4mo2F1dRVtv8yfND0hMYnZXClVNzGJ2VwgmZKYzMSCYrNT6syrs7KncRCVptbY6tB2tYtbeClXsOs3pPBcVVjQDERBl5g1L54tgsJg5NY/yQNMYNTiM9Kdbj1MFB5S4iQcM5x+6yOt7bUcayXeUs311OZX0LAIPTEsgf0Z8bc/tzck46E4emkxAb7XHi4KVyFxFPNba0smxXOX/fWsI/tpdQWNEAwLB+iZw3fhAzThjIaSMGkN0/MaKmVY6Xyl1E+lxtk4+3txzijY0HeXd7KfXNrSTFRXP6CRnMPfMEzszLYPjAZK9jhjSVu4j0icaWVv6+tYRFa4t5Z1sJTb42slLj+copwzhvwiBOP2Eg8TGaZgkUlbuI9BrnHGsLK3lpdRGvryumutFHZmo8s6fmctGJQ5iS279PTuiJRCp3EQm4msYWXltbzAsr9rHlQDWJsdHMnDSYr506jNNPyCBahd7rVO4iEjB7y+t46oM9vLSqiNomHxOHpvGfX53EJScNJTVBSxT7kspdRI7b2sJKfvPOTt7ccoiYKOPiE4dy7YzhnJzTTytcPKJyF5EeW7arnMfe2cn7O8tIT4zl22eP5poZwxmUluB1tIincheRY/bRvsP8Ysk2PthVTmZqPPfMGsdV04aTEq9KCRb6SoiI33YcquHnf93K21tLyEiJ476LJ3DVtFydKRqEVO4i0q3y2iZ+9dYOXvhwH0lx0dxxwViuP30EyRqpBy19ZUTkqFrbHM8t38sv/raN+uZWrp6Wy23n5DEwJd7raNINlbuIdGltYSU/em0DG/dX8y+jM/j3L08gb1Cq17HETyp3EfmU+mYfDy/ZxlMf7CEzJZ5fzz6Fi08coiWNIUblLiKfWLarnB++sp59FfXMmZ7LD2eO08lHIUrlLiI0+Vp5+I1tzH9/N7kDklhw83RmnDDQ61hyHFTuIhFux6EavrNgDVsP1nDN9OHcPWscSXGqhlCnr6BIhHLO8YeVhfz7ok2kxMfw5HX5nDN+kNexJEBU7iIRqKG5lR+9tpFXPiriX0Zn8MsrTiIrVVsGhBOVu0iE2V1WxzefXc32khpuOyeP756Tpy14w5DKXSSCvLejlG8//xHRUcZT35jKWWMyvY4kvUTlLhIBnHM89cEefvKXLYzOTGH+dfnkDEjyOpb0IpW7SJjztbbx4z9v4rnl+zhvwiAeueJk7d4YAfQVFglj9c0+vrtgDW9tKeGWs0bxwwvG6ZqlEULlLhKmymqbuPHpVWwoquSBSydyzYwRXkeSPqRyFwlDxZUNzJm/guKqBh6fM4XzJw72OpL0MZW7SJjZXVbHnPkrqG5o4dkbp3HaiAFeRxIPqNxFwsjWg9XMmf8hbc6xYO50Jg1L9zqSeCTKn4PMbKaZbTOznWZ2VxfP55rZO2a2xszWm9mswEcVkc+z9WA1Vz2xgugoWHiLij3SdVvuZhYNPAZcCEwAZpvZhCMO+xGw0Dl3CnAl8JtABxWRo/u42OOio/jD3BmMztJFNSKdPyP3qcBO51yBc64ZeBG49IhjHJDWcTsdKA5cRBH5PNsO1nDVEyuIjTYWzJ3OiIxkryNJEPBnzn0YUNjpfhEw7Yhjfgz8zcy+AyQD5wYknYh8rt1ldVw9v73YX5w7g5Eqdungz8i9qzMe3BH3ZwNPOeeygVnAs2b2mdc2s7lmtsrMVpWWlh57WhH5xMfLHduc4/mbpqvY5VP8KfciIKfT/Ww+O+1yI7AQwDm3DEgAMo58IefcPOdcvnMuPzNTGxaJ9FRZbRNznmxf7vjMDVMZnZXidSQJMv6U+0ogz8xGmlkc7X8wXXTEMfuAcwDMbDzt5a6huUgvqG3ycf3vP6S4soEnrz9Nq2KkS92Wu3POB9wKLAG20L4qZpOZ3W9ml3Qcdjtws5mtAxYA1zvnjpy6EZHj1NLaxree/4gtB2r4zdWnMnWkTlCSrvl1EpNzbjGw+IjH7ut0ezNwRmCjiUhnzjnu+eMGlm4v5cGvT+ZL43RJPDk6v05iEhHv/eqtHby0uojbzsnjitNyvY4jQU7lLhICXluzn0ff3sHl+dl879w8r+NICFC5iwS51XsPc+cr65k+agA/+cpkzLQfu3RP5S4SxIoO13PLs6sYmp7Ab6+eQlyMfmTFP9oVUiRI1TX5uOnpVTT52nhx7mn0T47zOpKEEA0DRIKQc447X17P9kM1PHbVqTpJSY6Zyl0kCP1uaQF/2XCAH84cx5ljdDa3HDuVu0iQWbq9lIfe2MrFJw5h7pmjvI4jIUrlLhJECivq+c6CNYwZlMpDl52olTHSYyp3kSDR5Gvl1hc+oq3N8ficKSTFab2D9Jy+e0SCxE//soV1RVU8PmeKLrghx00jd5Eg8Od1xTy9bC83/ctIZk4a7HUcCQMqdxGP7S6r465X1jNleH9+eOE4r+NImFC5i3ioydfKdxZ8RGxMFL+efQqx0fqRlMDQnLuIhx56Yxsb91cz75opDO2X6HUcCSMaJoh45O9bD/Hk+7u5bsZwzp+oeXYJLJW7iAdKqhv5wUvrGT8kjbtnjfc6joQhlbtIH3PO8YOX11Pf7OPXs08mITba60gShlTuIn3smWV7Wbq9lHsvmsDorFSv40iYUrmL9KEdh2r46eItfHFsJnOm6VJ50ntU7iJ9pNnXxm0vriUlPoaHLjtJ+8ZIr9JSSJE+8ujb29l8oJr51+aTmRrvdRwJcxq5i/SBNfsO89t/7OLy/GzOnTDI6zgSAVTuIr2sobmV2xeuY0h6Iv/v4glex5EIoWkZkV720JKtFJTV8cJN00hNiPU6jkQIjdxFetHygnJ+/889XH/6CE4fneF1HIkgKneRXlLf7OPOl9czfGASd84c63UciTCalhHpJQ8v2ca+inpenDtdV1WSPqeRu0gvWLWngqc+2MN1M4YzfdRAr+NIBFK5iwRYY0srd7y8nuz+idw5UxffEG/od0WRAHvkze3s7lgdkxyvHzHxhl8jdzObaWbbzGynmd11lGMuN7PNZrbJzF4IbEyR0LChqIon3ivgytNytDpGPNXtsMLMooHHgPOAImClmS1yzm3udEwecDdwhnPusJll9VZgkWDV0trGna+sJyMlXnu0i+f8GblPBXY65wqcc83Ai8ClRxxzM/CYc+4wgHOuJLAxRYLfvKUFbDlQzQNfmUR6ok5WEm/5U+7DgMJO94s6HutsDDDGzP5pZsvNbGagAoqEgl2ltTz69g5mTR7MBbpkngQBf/7a09W+pK6L18kDzgaygffMbJJzrvJTL2Q2F5gLkJurvawlPLS1Oe7+4wYSYqL48SUTvY4jAvg3ci8CcjrdzwaKuzjmT865FufcbmAb7WX/Kc65ec65fOdcfmZmZk8ziwSVl1YX8uHuCu6ZNZ6s1ASv44gA/pX7SiDPzEaaWRxwJbDoiGNeA74IYGYZtE/TFAQyqEgwKq1p4j//soWpIwdweX5O9/9BpI90W+7OOR9wK7AE2AIsdM5tMrP7zeySjsOWAOVmthl4B7jDOVfeW6FFgsX9r2+msaWNn351MlFRurKSBA+/zrBwzi0GFh/x2H2dbjvg+x3/RCLCP7aV8Od1xfzbuWMYnZXidRyRT9H2AyI90NDcyo9e28gJmcl88+xRXscR+QydGy3SA4++vYOiww38Ye504mOivY4j8hkauYsco60Hq5n/XgGX52czTTs+SpBSuYscg7Y2xz1/3EBaYix3X6gtBiR4qdxFjsGClfv4aF8l984aT//kOK/jiByVyl3ET6U1TTz4163MGDWQr5165A4cIsFF5S7ip58u3kJjSxs/+eokzLSmXYKbyl3EDx/sKuPVNfv55lmjOCFTa9ol+KncRbrR5Gtf0547IIlvfXG013FE/KJ17iLdmPduAQWldTz1jdNIiNWadgkNGrmLfI695XX8+p2dXDR5CGeP1QXGJHSo3EWOwjnHfX/aRFx0FPd9eYLXcUSOicpd5CgWbzjIu9tL+f55YxiUpn3aJbSo3EW6UNPYwv2vb2LCkDSunTHc6zgix0x/UBXpwiNv7qCkponH50whJlpjIAk9+q4VOcLG/VU89cFuZk/N5ZTc/l7HEekRlbtIJ21tjh+9tpH+SXH88IJxXscR6TGVu0gnL64sZG1hJfdeNJ70pFiv44j0mMpdpENZbRMPvrGV6aMG8NVTtDGYhDaVu0iHny3eSn2zj598RRuDSehTuYsAy3aV88pHRdz8hVGMzkr1Oo7IcVO5S8Rr9rXxo9c2kDMgke98Kc/rOCIBoXXuEvGeeK+AXaV1/P7600iM08ZgEh40cpeItq+8nv9+ewezJg/mi+O0MZiED5W7RCznHPct2khMlHHfxRO9jiMSUCp3iViLNxzkH9tK+f75Yxmcro3BJLyo3CUiVTe28OM/b2LSsDSu08ZgEob0B1WJSA+/sY3y2iaevC5fG4NJWNJ3tUScNfsO89yKvVw7YwQnZvfzOo5Ir1C5S0RpaW3jnlc3kpUaz+3nj/E6jkiv0bSMRJQn39/NlgPV/PbqU0lN0MZgEr78Grmb2Uwz22ZmO83srs857jIzc2aWH7iIIoGxr7yeX721nfMmDGLmpMFexxHpVd2Wu5lFA48BFwITgNlm9pmrBZtZKvBdYEWgQ4ocL+cc9762gWgz7r90ojYGk7Dnz8h9KrDTOVfgnGsGXgQu7eK4B4CHgMYA5hMJiD+tLea9HWXcOXMcQ9ITvY4j0uv8KfdhQGGn+0Udj33CzE4Bcpxzrwcwm0hAVNQ188Drmzk5px9zpmtNu0QGf/6g2tXvr+6TJ82igEeA67t9IbO5wFyA3Nxc/xKKHKcHXt9MVUMLz399MtFRmo6RyODPyL0IyOl0Pxso7nQ/FZgE/MPM9gDTgUVd/VHVOTfPOZfvnMvPzMzseWoRP/1jWwmvrtnPt84+gXGD07yOI9Jn/Cn3lUCemY00szjgSmDRx08656qccxnOuRHOuRHAcuAS59yqXkks4qfaJh/3vrqR0VkpfPtLo72OI9Knui1355wPuBVYAmwBFjrnNpnZ/WZ2SW8HFOmpXyzZRnFVAw9+fTLxMdqnXSKLXycxOecWA4uPeOy+oxx79vHHEjk+q/ZU8PSyPVw7fThThg/wOo5In9P2AxJ2GltaufPl9QxNT+TOmeO8jiPiCW0/IGHnl29up6CsjudvmkZyvL7FJTJp5C5h5aN9h5n/XgGzp+ZyxugMr+OIeEblLmHj4+mYwWkJ3DNL0zES2fQ7q4SNR97czs6SWp6+Yap2fJSIp5G7hIVVeyqY1zEdc9YYnSAnonKXkFff7OP2l9aR3T+Rey8a73UckaCgaRkJeT//61b2VdSz4ObppGh1jAigkbuEuKXbS3lm2V5uOGMk00cN9DqOSNBQuUvIOlzXzA9eWkdeVgp3XDDW6zgiQUW/w0pIcs5x9x83cLi+md9/4zQSYrV3jEhnGrlLSHppdRFvbDrID84fy8Sh6V7HEQk6KncJOXvL6/iPRZuYPmoAN31hlNdxRIKSyl1CSrOvje8uWEN0lPFfl5+sKyuJHIXm3CWk/NfftrGuqIrfXn0qw/rpQtciR6ORu4SMd7eX8rulBVw1LZcLJw/xOo5IUFO5S0goqWnk9oVrGTsolfsunuB1HJGgp2kZCXqtbY7bFqyltsnHCzdP17JHET+o3CXoPfLmdpYVlPPwZScyZlCq13FEQoKmZSSovbOthP95ZyeX52fzr/k5XscRCRkqdwla+ysb+Lc/rGXc4FTuv3SS13FEQorKXYJSY0sr33x2Nb5Wx2/nTNE8u8gx0py7BB3nHPe+upEN+6t44tp8RmYkex1JJORo5C5B55lle3nloyK+d24e500Y5HUckZCkcpegsqKgnAde38y54wfx3S/leR1HJGSp3CVo7C2v45vPrSZ3YBK/vOIkorRvjEiPqdwlKFQ3tnDj06toc/DkdaeRlhDrdSSRkKZyF8/5Wtu49YU17Cmr4/E5U/QHVJEA0GoZ8ZRzjvtf38zS7aX8/GuTmXGCroMqEggauYunHn+3gGeW7WXumaO4cmqu13FEwobKXTzz2pr9PPjGVr580lDumjnO6zgiYcWvcjezmWa2zcx2mtldXTz/fTPbbGbrzextMxse+KgSTv65s4w7Xl7H9FED+MW/nqiVMSIB1m25m1k08BhwITABmG1mR26ovQbId86dCLwMPBTooBI+1hZWMveZVYzKSOF31+QTH6OtBUQCzZ+R+1Rgp3OuwDnXDLwIXNr5AOfcO865+o67y4HswMaUcLHtYA3X//5DBqbE88yNU0lP1JJHkd7gT7kPAwo73S/qeOxobgT+ejyhJDztLa9jzpMriIuO4vmbpjEoLcHrSCJhy5+lkF1NhrouDzSbA+QDZx3l+bnAXIDcXK2MiCSFFfVc9cQKWlrbWHjLDHIGJHkdSSSs+TNyLwI6XyUhGyg+8iAzOxe4F7jEOdfU1Qs55+Y55/Kdc/mZmZk9ySshqOhwPbOfWE5NYwvP3jBNV1MS6QP+lPtKIM/MRppZHHAlsKjzAWZ2CvA72ou9JPAxJVQVHa7nynnLqW5o4fmbpjM5O93rSCIRodtyd875gFuBJcAWYKFzbpOZ3W9ml3Qc9jCQArxkZmvNbNFRXk4iyN7yuk+K/bmbpqnYRfqQX9sPOOcWA4uPeOy+TrfPDXAuCXHbDtZwzZPtc+wasYv0Pe0tIwG3rrCS637/IfExUSy8ZQZ5mmMX6XMqdwmod7eX8q3nVjMgJY7nb5xO7kCtihHxgvaWkYBZuLKQG55aSe7AZF7+5ukqdhEPaeQux805x6Nv7+BXb+3gC3kZ/ObqU0nVxTZEPKVyl+PS0NzKHS+v4/X1B7hsSjY/+9pkYqP1C6GI11Tu0mPFlQ3MfXYVm4qruevCcdxy5ijMtLujSDBQuUuPLC8o59YX1tDY0sr8a/M5Z/wgryOJSCcqdzkmbW2O3y0t4OElWxkxMJkXbtZ2AiLBSOUufjtc18wdL6/jrS0lXHTiEB78+omkxOtbSCQY6SdT/PL+jjJuf2ktFXXN/PjLE7ju9BGaXxcJYip3+VyNLa38Ysk25r+/m9FZKfzv9acxcai2EhAJdip3OarVew9z58vr2FVaxzXTh3PPrPEkxumSeCKhQOUun1Hf7OOXf9vOk//czdD0RJ65YSpnjtH++yKhROUun/K3TQf5jz9vZn9lA1dPy+WuC8fpbFOREKRyF6B97/UHXt/MW1tKGDsolYW3zGDqyAFexxKRHlK5R7iq+hZ+/fcdPL1sD7HRUdw7azzXnzFCWwiIhDiVe4RqbGnlueV7eeydnVQ2tHD5lBxuP38MWWkJXkcTkQBQuUeYZl8bC1cV8uu/7+BQdRNfyMvg7gvHM2FomtfRRCSAVO4RoqG5lRdX7mPe0gIOVDWSP7w/j155CtNHDfQ6moj0ApV7mCuvbeL5Fft4+oM9lNc1M3XEAH72tcmcNSZTZ5iKhDGVe5jaXFzN0x/s4dW1+2n2tXH22Ey+dfZorYARiRAq9zDS0NzKn9cX88KKfawtrCQhNorL87O5/vSRjM5K8TqeiPQhlXuIa2tzfLingldWF/HXjQepbfIxOiuF+y6ewNdOHUa/pDivI4qIB1TuIcg5x7qiKv6yvpjFGw6yv7KB5LhoZk0ewmVTspk6coDm00UinMo9RLS0tvHh7gre3HyINzcfYn9lA7HRxpl5mdxxwVgumDhYm3qJyCdU7kGsuLKBpdtLeXd7Ke/vLKOm0Ud8TBRfyMvgtnPzuGDCYNKTtO+LiHyWyj2IHKxqZOWeCpYVlLNsVzm7y+oAGJKewKxJQ/jS+Cy+kJdBUpy+bCLy+dQSHmn2tbH1YDVrCytZs6+SVXsrKKxoACA1PoapIwdw9bRczhyTSV5WiubQReSYqNz7QG2Tj20Ha9h6sJqN+6vZVFzF1gM1NLe2AZCREk/+8P5cN2MEp40YwMShacRo4y4ROQ4q9wBxzlFR18zusjoKSuvYWVrLzpJadpTUfDIiB0hPjGXi0DSuP2MEJ2X346ScdIb1S9TIXEQCSuV+DOqafBRXNlBU2cD+ww0UHW6gsKKefRX17C2vo7rR98mxcdFRjMpM5qTsflyRn8O4wWmMHZxKdn8VuYj0vogv97Y2R1VDC+V1zZTXNlFW20xpTSOltU0cqm7iUHUjh6obOVDVSE2n8gaIjTZy+ieRMyCJk3P6MSIjmVEZyYzISCanf6KmVkTEM36Vu5nNBB4FooH5zrmfH/F8PPAMMAUoB65wzu0JbNSuOedo8rVR2+SjrslHTaOP2iYftY0+qhtbqGn0Ud3QQlVDC5Uff6xv5nD9///Y2uY+87rRUUZWajxZqfEMH5jMjFEDGZyeyNB+CQzrl8iw/olkpSYQHaVRuIgEn27L3cyigceA84AiYKWZLXLObe502I3AYefcaDO7EngQuKI3Ai9cWcjjS3dR39RKXbOP+ubWLsv5SElx0aQnxpKeGEu/pFjyslLolxTHwOQ4BiTHMTAljoHJ8WSkxpGREs+ApDiiVNwiEqL8GblPBXY65woAzOxF4FKgc7lfCvy44/bLwP+YmTnnum/dY9Q/OY4JQ9JIiosmKS6GpLhokuNjSImPITk+htSEGFLjY0hJiCEtIZa0xFhS4mOIi9EUiYhEDn/KfRhQ2Ol+ETDtaMc453xmVgUMBMo6H2Rmc4G5ALm5uT0KfN6EQZw3YVCP/q+ISKTwZzjb1dzEkSNyf47BOTfPOZfvnMvPzMz0J5+IiPSAP+VeBOR0up8NFB/tGDOLAdKBikAEFBGRY+dPua8E8sxspJnFAVcCi444ZhFwXcfty4C/98Z8u4iI+KfbOfeOOfRbgSW0L4X8X+fcJjO7H1jlnFsEPAk8a2Y7aR+xX9mboUVE5PP5tc7dObcYWHzEY/d1ut0I/Gtgo4mISE9pfaCISBhSuYuIhCGVu4hIGDKvFrWYWSmw15NPfnwyOOLkrAgRie9b7zlyhNL7Hu6c6/ZEIc/KPVSZ2SrnXL7XOfpaJL5vvefIEY7vW9MyIiJhSOUuIhKGVO7Hbp7XATwSie9b7zlyhN371py7iEgY0shdRCQMqdyPg5n9wMycmWV4naW3mdnDZrbVzNab2atm1s/rTL3JzGaa2TYz22lmd3mdp7eZWY6ZvWNmW8xsk5nd5nWmvmJm0Wa2xsxe9zpLIKnce8jMcmi/9OA+r7P0kTeBSc65E4HtwN0e5+k1nS4teSEwAZhtZhO8TdXrfMDtzrnxwHTg2xHwnj92G7DF6xCBpnLvuUeAO+nioiThyDn3N+ecr+Puctr39Q9Xn1xa0jnXDHx8acmw5Zw74Jz7qON2De1lN8zbVL3PzLKBi4D5XmcJNJV7D5jZJcB+59w6r7N45Abgr16H6EVdXVoy7IvuY2Y2AjgFWOFtkj7xK9oHaW1eBwk0v7b8jURm9hYwuIun7gXuAc7v20S97/Pes3PuTx3H3Ev7r/DP92W2PubXZSPDkZmlAK8A33POVXudpzeZ2cVAiXNutZmd7XWeQFO5H4Vz7tyuHjezycBIYJ2ZQfv0xEdmNtU5d7APIwbc0d7zx8zsOuBi4Jwwv9KWP5eWDDtmFkt7sT/vnPuj13n6wBnAJWY2C0gA0szsOefcHI9zBYTWuR8nM9sD5DvnQmXToR4xs5nAL4GznHOlXufpTR3XAd4OnAPsp/1Sk1c55zZ5GqwXWftI5Wmgwjn3Pa/z9LWOkfsPnHMXe50lUDTnLv76HyAVeNPM1prZ414H6i0dfzj++NKSW4CF4VzsHc4ArgG+1PH1XdsxopUQpZG7iEgY0shdRCQMqdxFRMKQyl1EJAyp3EVEwpDKXUQkDKncRUTCkMpdRCQMqdxFRMJ8hsSHAAAAB0lEQVTQ/wFqiBP0W7Nw1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1. + T.exp(-z))\n",
    "    \n",
    "x = T.arange(-5, 5, .1)    \n",
    "y = logistic(x)\n",
    "\n",
    "plt.plot(x.numpy() ,y.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sigmoid outputs a value between $0$ and $1$,\n",
    "it's more reasonable to think of it as a probability.\n",
    "Note that an input of $0$ gives a value of $.5$. \n",
    "So in the common case, where we want to predict positive whenever the probability is greater than $.5$\n",
    "and negative whenever the probability is less than $.5$,\n",
    "we can just look at the sign of $\\boldsymbol{w}^T \\boldsymbol{x} + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss\n",
    "\n",
    "Now that we've got a model that outputs probabilities,\n",
    "we need to choose a loss function.\n",
    "When we wanted to predict *how much*, we used squared error $(y-\\hat{y})^2$,\n",
    "as our measure our model's performance. \n",
    "\n",
    "Since now we're thinking about outputing probabilities,\n",
    "one natural objective is to say that we should choose the weights \n",
    "that give the actual labels in the training data the highest probability.\n",
    "\n",
    "$$\\max_{\\theta} P_{\\theta}( (y_1, ..., y_n) | \\boldsymbol{x}_1,...,\\boldsymbol{x}_n )$$\n",
    "\n",
    "Because each example is independent of the others, and each label depends only on the features of the corresponding examples, we can rewrite the above as\n",
    "\n",
    "$$\\max_{\\theta} P_{\\theta}(y_1|\\boldsymbol{x}_1)P_{\\theta}(y_2|\\boldsymbol{x}_2) ... P(y_n|\\boldsymbol{x}_n)$$\n",
    "\n",
    "\n",
    "\n",
    "This function is a product over the examples, but in general, because we want to train by stochastic gradient descent, it's a lot easier to work with a loss function that breaks down as a sum over the training examples. \n",
    "\n",
    "$$\\max_{\\theta} \\log P_{\\theta}(y_1|\\boldsymbol{x}_1) + ... + \\log P(y_n|\\boldsymbol{x}_n)$$\n",
    "\n",
    "Because we typically express our objective as a *loss* we can just flip the sign, giving us the *negative log probability:*\n",
    "\n",
    "$$\\min_{\\theta} \\left(- \\sum_{i=1}^n \\log P_{\\theta}(y_i|\\boldsymbol{x}_i)\\right)$$\n",
    "\n",
    "If we interpret $\\hat{y_i}$ as the probability that the $i$-th example belongs to the positive class (i.e $y_i=1$),\n",
    "then $1 - \\hat{y_i}$ is the probability that the $i$-th example belongs to the negative class (i.e $y_i=0$). This is equivalent to saying\n",
    "\n",
    "$$P_{\\theta}(y_i|\\boldsymbol{x}_i) = \\begin{cases}\n",
    "    \\hat{y}_i, & \\text{if } y_i = 1\\\\\n",
    "    1-\\hat{y}_i, & \\text{if } y_i = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "which can be written in a more compact form\n",
    "\n",
    "$$ P_{\\theta}(y_i|\\boldsymbol{x}_i) = \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can express our learning objective as:\n",
    "\n",
    "$$\\ell (\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) =  - \\sum_{i=1}^n y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i)$$\n",
    "\n",
    "If you're learning machine learning for the first time, that might have been too much information too quickly. \n",
    "Let's take a look at this loss function and break down what's going on more slowly. \n",
    "The loss function consists of two terms, $y_i \\log \\hat{y}_i$ and $(1-y_i) \\log (1-\\hat{y}_i)$.\n",
    "Because $y_i$ only takes values $0$ or $1$, for a given data point, one of these terms disappears. \n",
    "When $y_i$ is $1$, this loss says that we should maximize $\\log \\hat{y}_i$, giving higher probability to the *correct* answer. \n",
    "When $y_i$ is $0$, this loss function takes value $\\log (1-\\hat{y}_i)$. That says that we should maximize the value $1-\\hat{y}$ which we already know is the probability assigned to $\\boldsymbol{x}_i$ belonging to the negative class.\n",
    "\n",
    "Note that this loss function is commonly called *log loss* and is also commonly referred to as *binary cross entropy*. It is a special case of negative log likelihood. And it is a special case of cross-entropy, which can apply to the multi-class ($>2$) setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

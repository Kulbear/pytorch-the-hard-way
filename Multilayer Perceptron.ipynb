{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "In the previous chapters we showed how you could implement multiclass logistic regression \n",
    "(also called *softmax regression*)\n",
    "for classifiying images in CIFAR10 dataset into 10 different classes.\n",
    "This is where things start to get fun.\n",
    "We understand how to wrangle data, \n",
    "coerce our outputs into a valid probability distribution,\n",
    "how to apply an appropriate loss function,\n",
    "and how to optimize over our parameters.\n",
    "Now that we've covered these preliminaries, \n",
    "we can extend our toolbox to include deep neural networks.\n",
    "\n",
    "Recall that before, we mapped our inputs directly onto our outputs through a single linear transformation.\n",
    "$$\\hat{y} = \\mbox{softmax}(W \\boldsymbol{x} + b)$$\n",
    "\n",
    "Graphically, we could depict the model like this, where the orange nodes represent inputs and the teal nodes on the top represent the output:\n",
    "<center>![](../img/simple-softmax-net.png)</center>\n",
    "\n",
    "If our labels really were related to our input data by an approximately linear function,\n",
    "then this approach might be adequate.\n",
    "*But linearity is a strong assumption*.\n",
    "Linearity means that given an output of interest,\n",
    "for each input,\n",
    "increasing the value of the input should either drive the value of the output up\n",
    "or drive it down,\n",
    "irrespective of the value of the other inputs.\n",
    "\n",
    "Imagine the case of classifying cats and dogs based on black and white images.\n",
    "That's like saying that for each pixel, \n",
    "increasing its value either increases the probability that it depicts a dog or decreases it.\n",
    "That's not reasonable. After all, the world contains both black dogs and black cats, and both white dogs and white cats. \n",
    "\n",
    "Teasing out what is depicted in an image generally requires allowing more complex relationships between\n",
    "our inputs and outputs, considering the possibility that our pattern might be characterized by interactions among the many features. \n",
    "In these cases, linear models will have low accuracy. \n",
    "We can model a more general class of functions by incorporating one or more *hidden layers*.\n",
    "The easiest way to do this is to stack a bunch of layers of neurons on top of each other.\n",
    "Each layer feeds into the layer above it, until we generate an output.\n",
    "This architecture is commonly called a \"multilayer perceptron\".\n",
    "With an MLP, we're going to stack a bunch of layers on top of each other.\n",
    "\n",
    "$$h_1 = \\phi(W_1\\boldsymbol{x} + b_1)$$\n",
    "$$h_2 = \\phi(W_2\\boldsymbol{h_1} + b_2)$$\n",
    "$$...$$\n",
    "$$h_n = \\phi(W_n\\boldsymbol{h_{n-1}} + b_n)$$\n",
    "\n",
    "Note that each layer requires its own set of parameters.\n",
    "For each hidden layer, we calculate its value by first applying a linear function \n",
    "to the activations of the layer below, and then applying an element-wise\n",
    "nonlinear activation function. \n",
    "Here, we've denoted the activation function for the hidden layers as $\\phi$.\n",
    "Finally, given the topmost hidden layer, we'll generate an output.\n",
    "Because we're still focusing on multiclass classification, we'll stick with the softmax activation in the output layer.\n",
    "\n",
    "$$\\hat{y} = \\mbox{softmax}(W_y \\boldsymbol{h}_n + b_y)$$\n",
    "\n",
    "Graphically, a multilayer perceptron could be depicted like this:\n",
    "\n",
    "<center>![](../img/multilayer-perceptron.png)</center>\n",
    "\n",
    "Multilayer perceptrons can account for complex interactions in the inputs because \n",
    "the hidden neurons depend on the values of each of the inputs. \n",
    "It's easy to design a hidden node that that does arbitrary computation,\n",
    "such as, for instance, logical operations on its inputs.\n",
    "And it's even widely known that multilayer perceptrons are universal approximators. \n",
    "That means that even for a single-hidden-layer neural network,\n",
    "with enough nodes, and the right set of weights, it could model any function at all!\n",
    "Actually learning that function is the hard part. \n",
    "And it turns out that we can approximate functions much more compactly if we use deeper (vs wider) neural networks.\n",
    "We'll get more into the math in a subsequent chapter, but for now let's actually build an MLP.\n",
    "In this example, we'll implement a multilayer perceptron with two hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PyTorch and its related packages\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# set default device based on CUDA's availability\n",
    "device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CIFAR10 dataset\n",
    "\n",
    "This time we're going to work with real data, each a 32 by 32 by 3 small RGB image contains a specific object.\n",
    "\n",
    "To start, we'll use PyTorch's utility for grabbing a copy of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (32, 32, 3)\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = T.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = T.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts of the dataset for training and testing. Each part has N items and each item is a tuple of an image and a label.\n",
    "\n",
    "Note that each image has been formatted as a 3-tuple (height, width, channel). For color images, the channel would have 3 dimensions (red, green and blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Mathematically, that's a perfectly reasonable thing to do. However, computationally, things can get hairy. We'll revisit the issue at length in a chapter more dedicated to implementation and less interested in statistical modeling. But we're going to make a change here so we want to give you the gist of why.\n",
    "\n",
    "Recall that the softmax function calculates $\\hat y_j = \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}$, where $\\hat y_j$ is the j-th element of the input ``yhat`` variable in function ``cross_entropy`` and $z_j$ is the j-th element of the input ``y_linear`` variable in function ``softmax``\n",
    "\n",
    "If some of the $z_i$ are very large (i.e. very positive), $e^{z_i}$ might be larger than the largest number we can have for certain types of ``float`` (i.e. overflow). This would make the denominator (and/or numerator) ``inf`` and we get zero, or ``inf``, or ``nan`` for $\\hat y_j$. In any case, we won't get a well-defined return value for ``cross_entropy``. This is the reason we subtract $\\text{max}(z_i)$ from all $z_i$ first in ``softmax`` function. You can verify that this shifting in $z_i$ will not change the return value of ``softmax``.\n",
    "\n",
    "After the above subtraction/ normalization step, it is possible that $z_j$ is very negative. Thus, $e^{z_j}$ will be very close to zero and might be rounded to zero due to finite precision (i.e underflow), which makes $\\hat y_j$ zero and we get ``-inf`` for $\\text{log}(\\hat y_j)$. A few steps down the road in backpropagation, we starts to get horrific not-a-number (``nan``) results printed to screen.\n",
    "\n",
    "Our salvation is that even though we're computing these exponential functions, we ultimately plan to take their log in the cross-entropy functions. It turns out that by combining these two operators ``softmax`` and ``cross_entropy`` together, we can elude the numerical stability issues that might otherwise plague us during backpropagation. As shown in the equation below, we avoided calculating $e^{z_j}$ but directly used $z_j$ due to $log(exp(\\cdot))$.\n",
    "$$\\text{log}{(\\hat y_j)} = \\text{log}\\left( \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}\\right) = \\text{log}{(e^{z_j})}-\\text{log}{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)} = z_j -\\text{log}{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)}$$\n",
    "\n",
    "We'll want to keep the conventional softmax function handy in case we ever want to evaluate the probabilities output by our model. But instead of passing softmax probabilities into our new loss function, we'll just pass our ``yhat_linear`` and compute the softmax and its log all at once inside the softmax_cross_entropy loss function, which does smart things like the log-sum-exp trick ([see on Wikipedia](https://en.wikipedia.org/wiki/LogSumExp)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model with non-linear activation functions\n",
    "\n",
    "If we compose a multi-layer network but use only linear operations, then our entire network will still be a linear function. That's because $\\hat{y} = X \\cdot W_1 \\cdot W_2 \\cdot W_2 = X \\cdot W_4 $ for $W_4 = W_1 \\cdot W_2 \\cdot W3$. To give our model the capacity to capture nonlinear functions, we'll need to interleave our linear operations with activation functions. In this case, we'll use the rectified linear unit (ReLU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = image_size[0] * image_size[1] * image_size[2]\n",
    "n_h1 = 512\n",
    "n_o = 10\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultilayerPerceptron, self).__init__(**kwargs)\n",
    "        self.dense_1 = T.nn.Linear(n_in, n_h1)\n",
    "        self.dense_2 = T.nn.Linear(n_h1, n_o)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, n_in)\n",
    "        # apply rectified linear unit (which is non-linear!)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dense_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultilayerPerceptron()\n",
    "net.to(device)\n",
    "\n",
    "# also the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n",
      "Train Acc. => 51.468% | Train Loss => 1.40425\n",
      "Test Acc.  => 50.320% | Test Loss  => 1.41771\n",
      "\n",
      "Epoch: 8\n",
      "Train Acc. => 58.618% | Train Loss => 1.21203\n",
      "Test Acc.  => 52.780% | Test Loss  => 1.34238\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(1, 33):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = .0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if device == 'cuda':\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # reset the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = T.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    running_loss /= len(train_loader)\n",
    "    train_loss.append(running_loss)\n",
    "    running_acc = correct / total\n",
    "    train_acc.append(running_acc)\n",
    "    \n",
    "    if epoch % 4 == 0:\n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Train Acc. => {:.3f}%'.format(100 * running_acc), end=' | ')\n",
    "        print('Train Loss => {:.5f}'.format(running_loss))\n",
    "    \n",
    "    # evaluate on the test set\n",
    "    # note this is usually performed on the validation set\n",
    "    # for simplicity we just evaluate it on the test set\n",
    "    with T.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_running_loss = .0\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            if device == 'cuda':\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = T.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_running_loss /= len(test_loader)\n",
    "        test_loss.append(test_running_loss)\n",
    "        test_running_acc = correct / total\n",
    "        test_acc.append(test_running_acc)\n",
    "        \n",
    "        if epoch % 4 == 0:\n",
    "            print('Test Acc.  => {:.3f}%'.format(100 * test_running_acc), end=' | ')\n",
    "            print('Test Loss  => {:.5f}'.format(test_running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice! With just two hidden layers containing 256 hidden nodes, respectively, we can achieve over 54% accuracy on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(test_loss, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(test_acc, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
